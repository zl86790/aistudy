{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b5ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate                  1.9.0\n",
      "datasets                    4.0.0\n",
      "intel_extension_for_pytorch 2.7.0\n",
      "llmcompressor               0.4.1\n",
      "torch                       2.7.1\n",
      "transformers                4.39.3\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E \"torch|transformers|accelerate|llm-compressor|datasets|optimum|llmcompressor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c949ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 11 14:36:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.144                Driver Version: 570.144        CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 6000                Off |   00000000:B3:00.0  On |                  Off |\n",
      "| 33%   39C    P8             27W /  260W |     529MiB /  24576MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3734      G   /usr/lib/xorg/Xorg                      269MiB |\n",
      "|    0   N/A  N/A            3887      G   ...c/gnome-remote-desktop-daemon          3MiB |\n",
      "|    0   N/A  N/A            3925      G   /usr/bin/gnome-shell                    127MiB |\n",
      "|    0   N/A  N/A            4568      G   .../6638/usr/lib/firefox/firefox        125MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU环境\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44927ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 应输出True\n",
    "print(torch.version.cuda)  # 查看CUDA版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fa026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始模型（未量化）\n",
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"/home/pactera/aistudy/week03/opt-2.7b\"  # 保持原模型路径\n",
    "\n",
    "# 使用GPU加载原始OPT-2.7b模型\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    device=0,  # 使用第0块GPU\n",
    "    do_sample=True,\n",
    "    num_return_sequences=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a57a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The man worked as a bouncer for the same establishment in Boston the night of the first shooting and'},\n",
       " {'generated_text': 'The man worked as a manager for a couple of companies, who know his name. Just like the'},\n",
       " {'generated_text': 'The man worked as a mechanic for a car dealership. His customers, some of whom thought their vehicles'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始模型生成测试（男性职业）\n",
    "generator(\"The man worked as a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2532d",
   "metadata": {},
   "source": [
    "# 使用 LLM.int8 () 量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4194e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# 使用 LLM.int8() 量化模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 定义量化路径\n",
    "quant_path = \"models/opt-2.7b-int8\"\n",
    "\n",
    "# 加载并量化模型（LLM.int8()核心配置）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,  # 启用8bit量化（LLM.int8()的基础）\n",
    "    torch_dtype=torch.float16,\n",
    "    # LLM.int8()会自动处理离群点，无需额外配置\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # 确保pad_token存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4121a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化模型已保存至：models/opt-2.7b-int8\n"
     ]
    }
   ],
   "source": [
    "# 保存量化模型和分词器\n",
    "model.save_pretrained(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "print(f\"量化模型已保存至：{quant_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eaa614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "# 加载并测试LLM.int8()量化模型\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 加载量化后的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    quant_path,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,  # 加载8bit量化模型\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8e284e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成函数\n",
    "def generate_text(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)  # 确保输入与模型在同一设备\n",
    "    \n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96858906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to see you're doing well. (I hope I'm not too late!)  I'm currently working on a story. I have to take a bit of a break because my little one is teething, but I'm back at it and I'm actually feeling better about it.   What's your favorite Christmas\n",
      "The woman worked as a beautician in the town of Leominster in Western Massachusetts, where she was known to many customers as \"The Lady of Leominster.\"\n",
      "\n",
      "Police say she was killed in a crash involving a car she was driving.\n",
      "\n",
      "Investigators say that the car was driving at an abnormally fast rate of speed when\n",
      "The man worked as a construction worker, but he still had enough money to buy this.\n",
      "Not really, it's just a joke. The man doesn't think he's doing anything wrong.\n",
      "The joke is that he's poor and buying a Lamborghini. A joke about the poor can't be funny since it's a stereotype.\n"
     ]
    }
   ],
   "source": [
    "# 量化模型生成测试（节日问候）\n",
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)\n",
    "\n",
    "# 量化模型生成测试（女性职业）\n",
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)\n",
    "\n",
    "# 量化模型生成测试（男性职业）\n",
    "result = generate_text(\"The man worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3651dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aistudy5)",
   "language": "python",
   "name": "aistudy5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
