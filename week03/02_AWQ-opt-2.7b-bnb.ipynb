{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f392859e-2772-4764-ad13-962c54fbeee4",
   "metadata": {},
   "source": [
    "# Transformers 模型量化技术：AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce87dba-e543-4329-a5ae-d83febee50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate                0.34.2\n",
      "autoawq                   0.2.9\n",
      "bitsandbytes              0.46.1\n",
      "datasets                  4.0.0\n",
      "optimum                   1.27.0\n",
      "torch                     2.7.0\n",
      "torchaudio                2.7.0\n",
      "torchcodec                0.5\n",
      "torchvision               0.22.0\n",
      "transformers              4.54.1\n"
     ]
    }
   ],
   "source": [
    "# 或者一次性列出所有已安装包及其版本（可搜索关键词）\n",
    "!pip list | grep -E \"torch|transformers|accelerate|bitsandbytes|datasets|optimum|awq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1673c1-0270-40d1-8968-b39b923b27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  6 12:37:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.144                Driver Version: 570.144        CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 6000                Off |   00000000:B3:00.0  On |                  Off |\n",
      "| 33%   39C    P5             22W /  260W |     641MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2925      G   /usr/lib/xorg/Xorg                      315MiB |\n",
      "|    0   N/A  N/A            3083      G   ...c/gnome-remote-desktop-daemon          3MiB |\n",
      "|    0   N/A  N/A            3124      G   /usr/bin/gnome-shell                    126MiB |\n",
      "|    0   N/A  N/A            4118      G   .../6565/usr/lib/firefox/firefox        161MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054a4a56-8d87-4e27-9ee7-86d886bf5105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 应输出True\n",
    "print(torch.version.cuda)  # 应与nvcc -V显示的版本一致（或兼容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008bd982-9fa5-4ef7-a7e8-8193dd02409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a1481a-3e00-475c-84d7-104c2d080e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"/home/pactera/aistudy/week03/opt-2.7b\"\n",
    "\n",
    "# 使用 GPU 加载原始的 OPT-2.7b 模型\n",
    "generator = pipeline('text-generation',\n",
    "                     model=model_path,\n",
    "                     device=0,\n",
    "                     do_sample=True,\n",
    "                     num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209bae3a-3ade-4fb4-9a0f-f0663d5af4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=21) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The woman worked as a sex worker. I mean, that's kind of a dealbreaker.\\nWhat about a hooker?\\nI'm not sure, I'm not into that sort of thing.\"},\n",
       " {'generated_text': 'The woman worked as a nurse at a private care home for the elderly in Auckland, New Zealand, and her employer has been slammed for allegedly failing to protect her from a colleague who was verbally and physically abusive and then repeatedly raped her.\\n\\nPuja Gupta, 36, from Auckland, had been working at the home for the past six years and was being trained to become a registered nurse when she was allegedly raped by a colleague.\\n\\nPolice are investigating the circumstances surrounding the incident and have referred the case to the Health and Disability Commissioner (HDC).\\n\\nWhile she was being raped, Gupta suffered an epileptic fit and was taken to hospital where she was diagnosed with epilepsy. She was discharged the next day to recover at home.\\n\\nThe Herald on Sunday reports that Gupta, a mother of two, suffered a seizure as a result of her attack and was taken to hospital with a brain bleed.\\n\\nShe has since been diagnosed with epilepsy, post-traumatic stress disorder and anxiety.\\n\\nThe attack left her feeling “trapped” in her body and unable to move.\\n\\nGupta told the Herald on Sunday: “I didn’t know I was being raped. I just thought I was having a seizure.\\n\\n“I felt'},\n",
       " {'generated_text': 'The woman worked as a waitress in a restaurant in the centre of Leeds. (Picture: SWNS)\\n\\nA waitress who was stabbed to death in a knife attack after a row over a missed tip has been remembered as ‘funny and caring’ by her friends.\\n\\nMariya Yefremova, 20, was stabbed in a bar in Leeds city centre on Friday night as she helped customers.\\n\\nShe suffered a single knife wound to the chest and died at Leeds General Infirmary on Monday afternoon.\\n\\nThe restaurant where she worked in the city centre was closed following the attack.\\n\\nHer friends said she was a devoted daughter and sister.\\n\\nIn a tribute posted on Facebook, her sister-in-law said: ‘My sister was very kind and loving. She was a great person, always helping people. She was a caring sister-in-law, who would do anything for anyone.\\n\\n‘She was a very beautiful, funny and caring woman. I always smile when I think of her.’\\n\\nAnother friend said: ‘RIP Mariya. You will never be forgotten.’\\n\\nA man, aged in his 30s, was arrested and later released on bail.\\n\\nDet'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The woman worked as a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e9868c-d268-456f-9528-d10e6e38452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=21) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The man worked as a mechanic, a police officer and a doctor, and he was a devoted father and husband.\\n\\nBut as the years went on, he lost his appetite for life. He told his wife he was tired of being sick. He had been diagnosed with cancer.\\n\\n“He was getting no younger,” said his wife, Nancy. “He didn’t want to get old.”\\n\\nSo on Friday, the family found out that James E. “Jim” Martin, 76, died peacefully at home on the outskirts of Dallas, surrounded by his wife, two kids, and three grandchildren.\\n\\nMartin was one of three Dallas men who died unexpectedly in recent weeks.\\n\\nThe Dallas police officer who died last week was a beloved veteran, well-liked by his colleagues and friends.\\n\\nAnd earlier this month, a longtime lawyer who was known for his work on behalf of the elderly and the terminally ill died suddenly from a heart attack.\\n\\nThe causes of death for all three men remain under investigation.\\n\\nBut the circumstances of each man’s death have raised concerns among their friends and family.\\n\\nMartin, the mechanic, worked at the Dallas Police Department for 41 years, beginning'},\n",
       " {'generated_text': 'The man worked as a bouncer during the day and at night he was a heroin dealer.\\n\\nHis name was James Edward Brown, and he had a troubled past that included jail time and convictions for drug dealing.\\n\\nBut in the early hours of December 1, he met a 29-year-old woman, gave her a kiss and then died of a heroin overdose.\\n\\nHis body was found in a pool of blood next to a dumpster in the back alley of his home in the 200 block of South Park Street, an alley that leads to the North Avenue subway station.\\n\\nBrown, who was 43, had no obvious signs of trauma other than heroin in his blood.\\n\\nThe woman was found dead in a second-floor apartment on the block, the Star has learned.\\n\\nPolice have not said whether they were investigating the deaths as a possible double homicide, but in both cases a single woman was found in the first floor apartment and a man was found in the alleyway.\\n\\nThe discovery of a male body in the alley is a “very unusual one,” says Toronto Police homicide Det.-Sgt. Gary Giroux.\\n\\n“Typically if you find a body in an alleyway, you’re not going to'},\n",
       " {'generated_text': 'The man worked as a cab driver and lived in a tent for months before he could afford to rent a room.\\n\\nHe says the government has been ‘trying to get rid of us for a long time’\\n\\nA man who’s lived in a tent for months outside the Supreme Court of Canada in Ottawa says he has no intention of leaving until the federal government gets serious about looking after Indigenous people.\\n\\n“We’re not going anywhere,” said Shawn Alexander.\\n\\nAlexander, 37, is one of several Indigenous people who have been camping outside the Supreme Court since November.\\n\\nThe group is protesting the federal government’s decision to end the First Nations and Inuit land claim program.\\n\\n“We’ve been here since November. The government is trying to get rid of us for a long time,” said Alexander.\\n\\nThe federal government made the announcement in November, saying the program had failed to deliver benefits to First Nations and Inuit.\\n\\n“We’re here to make it clear that we’re not going anywhere,” said Alexander.\\n\\nAlexander says he has been living in a tent as a way to draw attention to what he calls the Canadian'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The man worked as a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485bbfc-cc40-4cbe-9329-2a6ddcb8305d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82dad7ad-1b0c-44d3-906e-a238a38e4e98",
   "metadata": {},
   "source": [
    "# 使用 AutoAWQ 量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7366a13d-cfb2-4a4c-a8a2-1031b01923a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "quant_path = \"models/opt-2.7b-bab\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e208daaf-5b15-48cf-88b8-50807579aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/opt-2.7b-bab/tokenizer_config.json',\n",
       " 'models/opt-2.7b-bab/special_tokens_map.json',\n",
       " 'models/opt-2.7b-bab/vocab.json',\n",
       " 'models/opt-2.7b-bab/merges.txt',\n",
       " 'models/opt-2.7b-bab/added_tokens.json',\n",
       " 'models/opt-2.7b-bab/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcb47f-57e2-4d93-91a8-209b13520b8f",
   "metadata": {},
   "source": [
    "# 使用 GPU 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49ca456-2e99-4f86-af20-5ac40f9b0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605051ed-339e-4103-9083-6c6e0135eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e560b3f-2db6-4a2b-adc8-bb91a5c3a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to see you're still around.\n",
      "I'm still here, just not posting as much. I'm still here though.\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9346a0ba-6d89-4082-9caa-9a74dac8c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a nurse at the hospital and was a member of the hospital's staff.\n",
      "\n",
      "The woman was a member of the hospital's staff and had been working at the hospital for about a year.\n",
      "\n",
      "The woman was a member of the hospital's staff and had been working at the hospital for about a year.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca0e14-5562-4ad1-8ccf-3f33292c62b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09d0ca-b4d4-4d34-b66a-1247ffea55f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
