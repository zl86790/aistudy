{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2424889a-99d6-4219-9a52-c6a8e780d554",
   "metadata": {},
   "source": [
    "# PEFT 库 LoRA 实战 - OpenAI Whisper-large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0f581d-2916-4a10-baf6-f1af1da21a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用，当前 GPU 数量: 1\n",
      "当前使用的 GPU: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入 PyTorch 库\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 可用，当前 GPU 数量: {torch.cuda.device_count()}\")  # 输出可用 GPU 的数量\n",
    "    print(f\"当前使用的 GPU: {torch.cuda.get_device_name(0)}\")      # 输出当前使用的 GPU 名称\n",
    "else:\n",
    "    print(\"未检测到可用的 GPU\")  # 没有检测到可用的 GPU 时输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9352e8d-9b6d-4b9f-8954-3e4bffaa29ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27f0174-92d4-4e0a-9a5d-d27f35be3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.enabled)  # True 表示 cuDNN 可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5060eed4-4373-46e1-ad95-b63d2e9ba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8-ja\"\n",
    "\n",
    "language = \"Japanese\"\n",
    "language_abbr = \"ja\"    \n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
    "\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965ae42e-0e74-4e11-ac3d-f9db5cf7def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006c45bc-8b87-4bb2-96cd-c5f7f995bc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 7071\n",
      "训练集第一条数据： {'client_id': '397d4d8526a74d582e018bfa9adb687127eefc5a039c14c4a409cfbefa682e99e6466418aa155a6c1ebe1a811a29cf75fe5d9e9c56faff87bdcf6002b0bc8668', 'path': 'ja_train_0/common_voice_ja_35210899.mp3', 'audio': <datasets.features._torchcodec.AudioDecoder object at 0x7f8d36cbf590>, 'sentence': '山口県防府市', 'up_votes': 2, 'down_votes': 0, 'age': 'twenties', 'gender': 'other', 'accent': '', 'locale': 'ja', 'segment': '', 'variant': ''}\n",
      "验证集大小： 4961\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/mnt/e/aistudy_workspace/week02/ja_dataset/train/*.parquet\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "common_voice[\"validation\"] = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/mnt/e/aistudy_workspace/week02/ja_dataset/validation/*.parquet\",\n",
    "    split=\"train\"  # 这里用 train 标记，后续通过 DatasetDict 区分\n",
    ")\n",
    "\n",
    "# 验证加载结果\n",
    "print(\"训练集大小：\", len(common_voice[\"train\"]))\n",
    "print(\"训练集第一条数据：\", common_voice[\"train\"][0])\n",
    "print(\"验证集大小：\", len(common_voice[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e009b88-073b-46d3-9a62-ba6a4596ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # 检查是否支持 GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae77d34-b651-424f-9d20-c170f3655771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 7071\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 4961\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc948887-b9a1-465f-aa0f-ebbe5299481d",
   "metadata": {},
   "source": [
    "# 预处理训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9631b6-253b-4838-9204-13630ddef9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# 从预训练模型加载特征提取器\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 从预训练模型加载分词器，可以指定语言和任务以获得最适合特定需求的分词器配置\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "# 从预训练模型加载处理器，处理器通常结合了特征提取器和分词器，为特定任务提供一站式的数据预处理\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf8c7b9-dd60-4cbe-9de6-9997bc71ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5719c8a-5c8f-44bb-86d2-be4437cdd128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': <datasets.features._torchcodec.AudioDecoder at 0x7f8e8013c950>,\n",
       " 'sentence': '山口県防府市',\n",
       " 'variant': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7aea6-8206-40c8-8ade-0027f142f949",
   "metadata": {},
   "source": [
    "## 降采样音频数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1815512e-7810-4d2f-85e1-d011058e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0380f4-9b60-4f02-8d04-32bce2dcff0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': <datasets.features._torchcodec.AudioDecoder at 0x7f8d020b71d0>,\n",
       " 'sentence': '山口県防府市',\n",
       " 'variant': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling_rate 从 48KHZ 降为 16KHZ\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cc9be-1657-43e9-b721-fd4b4ece997e",
   "metadata": {},
   "source": [
    "## 整合以上数据处理为一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a777788f-a223-426e-beb2-31ab9e0765b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae32314-a6d1-4c08-97bb-eef4a6c7c8c9",
   "metadata": {},
   "source": [
    "## 数据抽样（演示需要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde6ac80-c061-4b1a-bfd7-3176b8b6fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_common_voice = DatasetDict()\n",
    "\n",
    "small_common_voice[\"train\"] = common_voice[\"train\"].shuffle(seed=16).select(range(100))\n",
    "small_common_voice[\"validation\"] = common_voice[\"validation\"].shuffle(seed=16).select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a33dc3-6392-4587-b2b1-c2aa4a44e161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a0683-c5f1-4df4-be18-200624bec6cc",
   "metadata": {},
   "source": [
    "## 如果全量训练，则使用完整数据代替抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a7a486f-00eb-45b3-a185-c0d980ffcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据处理\n",
    "tokenized_common_voice = small_common_voice.map(prepare_dataset)\n",
    "\n",
    "# 完整数据训练，尝试开启 `num_proc=16` 参数多进程并行处理（如阻塞无法运行，则不使用此参数）\n",
    "# tokenized_common_voice = common_voice.map(prepare_dataset, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44195316-1e5d-4512-b6ba-83f11bfc1d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant', 'input_features', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant', 'input_features', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a7ead-eee7-4266-a5eb-464cf957d147",
   "metadata": {},
   "source": [
    "# 自定义语音数据整理器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe9f26fa-28ab-4cfe-a561-f01e98f3a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# 定义一个针对语音到文本任务的数据整理器类\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # 处理器结合了特征提取器和分词器\n",
    "\n",
    "    # 整理器函数，将特征列表处理成一个批次\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 从特征列表中提取输入特征，并填充以使它们具有相同的形状\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 从特征列表中提取标签特征（文本令牌），并进行填充\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 使用-100替换标签中的填充区域，-100通常用于在损失计算中忽略填充令牌\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # 如果批次中的所有序列都以句子开始令牌开头，则移除它\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # 将处理过的标签添加到批次中\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch  # 返回最终的批次，准备好进行训练或评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8faee4-eb38-461c-ad5a-bce0ac71a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用给定的处理器实例化数据整理器\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39715e19-e63d-486a-853d-fb9255133a54",
   "metadata": {},
   "source": [
    "# 模型准备\n",
    "### 加载预训练模型（int8 精度）\n",
    "### 使用 int8  精度加载预训练模型，进一步降低显存需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f81ef1b8-9289-455c-84ae-c7cd5aa0f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=False, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9026c43f-5b5a-4a13-8882-1dedb250e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df59164c-871b-44cf-b13d-5cfe6fc6750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型配置中的forced_decoder_ids属性为None\n",
    "model.config.forced_decoder_ids = None  # 这通常用于指定在解码（生成文本）过程中必须使用的特定token的ID，设置为None表示没有这样的强制要求\n",
    "\n",
    "# 设置模型配置中的suppress_tokens列表为空\n",
    "model.config.suppress_tokens = []  # 这用于指定在生成过程中应被抑制（不生成）的token的列表，设置为空列表表示没有要抑制的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab0193-4857-497a-88a2-5c9a45994438",
   "metadata": {},
   "source": [
    "# PEFT 微调前的模型处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bb5973f-0ecf-4db7-90b9-3304b57f2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aistudy5/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550581a-45f8-4ac4-9fd1-7765269045c8",
   "metadata": {},
   "source": [
    "# LoRA Adapter 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4edaef1b-f522-4560-9eff-9112daf8ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# 创建一个LoraConfig对象，用于设置LoRA（Low-Rank Adaptation）的配置参数\n",
    "config = LoraConfig(\n",
    "    r=4,  # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=64,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影。\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",  # 设置bias的使用方式，这里没有使用bias\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b47dbd-2a93-4a32-9e75-5c7c71799a62",
   "metadata": {},
   "source": [
    "### 使用get_peft_model函数和给定的配置来获取一个PEFT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61114531-62ee-4ae7-b8db-f4a495756bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2d7e89a-c2cf-488b-acf1-23eaba09480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.12723204856023188\n"
     ]
    }
   ],
   "source": [
    "# 打印 LoRA 微调训练的模型参数\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad7593-b785-4611-835b-594c415d19fb",
   "metadata": {},
   "source": [
    "# 模型训练\n",
    "## Seq2SeqTrainingArguments 训练参数\n",
    "关于设置训练步数和评估步数\n",
    "\n",
    "基于 epochs 设置：\n",
    "\n",
    "    num_train_epochs=3,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "基于 steps 设置：\n",
    "\n",
    "    max_steps=100, # 训练总步数\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=25, # 评估步数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aec95b2a-f47b-458e-9c5e-b94c15700e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aistudy5/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    num_train_epochs=1,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    # warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批量大小\n",
    "    generation_max_length=32,  # 生成任务的最大长度\n",
    "    logging_steps=10,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=25,\n",
    "    fp16=True,  \n",
    "    fp16_opt_level=\"O1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971b388-9582-4221-83fc-d2847f3dcb05",
   "metadata": {},
   "source": [
    "## 实例化 Seq2SeqTrainer 训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f3be041-60b8-4ca8-88cd-9424a61f5be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aistudy5/lib/python3.11/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor, \n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505aa2cd-89ca-40b5-af80-b89713b6181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 34:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.345000</td>\n",
       "      <td>0.983045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13, training_loss=2.0008517045241137, metrics={'train_runtime': 2308.7898, 'train_samples_per_second': 0.043, 'train_steps_per_second': 0.006, 'total_flos': 2.126002176e+17, 'train_loss': 2.0008517045241137, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfc2c9-2aef-4983-acc3-fe750a5a6bf7",
   "metadata": {},
   "source": [
    "# 保存 LoRA 模型(Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7929730a-b61a-489c-9345-e4045f15c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28172a66-7ea5-4cd8-add6-20ffb52f2035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db610e4a-822c-4125-be32-78d893a1ddd6",
   "metadata": {},
   "source": [
    "# 模型推理（可能需要重启 Notebook）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e46869b3-1b0c-40f2-be5c-f18ff3839c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/whisper-large-v2-asr-int8-fr\"\n",
    "\n",
    "language = \"French\"          \n",
    "language_abbr = \"fr\"      \n",
    "language_decode = \"french\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc63d5-767d-4604-993b-bb29eebdc13b",
   "metadata": {},
   "source": [
    "## 使用 PeftModel 加载 LoRA 微调后 Whisper 模型\n",
    "### 使用 PeftConfig 加载 LoRA Adapter 配置参数，使用 PeftModel 加载微调后 Whisper 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0f7c089-bc73-40dd-9c91-1d6351dfd641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=False, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a083f085-d57d-498c-8978-ccbd62e40acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59f4c9-a351-4aab-abbe-3959d618848a",
   "metadata": {},
   "source": [
    "### 使用 Pipeline API 部署微调后 Whisper 实现中文语音识别任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b2bb661-1e9f-47d4-96cd-ce51620c0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = \"ja_test.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dc2f363-5b13-472a-93d1-01446708e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=peft_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7b8abb3-7d31-4ece-ab27-8e6e674ddc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_454/3500902550.py:3: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, max_new_tokens=255)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fa4b24d-4f9c-40c7-b880-8beb33251249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008時半後でロシアのカムチャツカ半島付近で起きたマグニチュード8.8の大きな地震。震源地に近い地域では大きな揺れが観測されました。ロイター通信はロシアの地元当局の話として幼稚園の建物に被害が出ていると伝えています。また、空港などで数人が軽傷を負ったということです。AP通信は2011年3月に起きた東日本大震災以来、世界最大規模と見られる。世界的にこれより強い地震はこれまで数回しか観測されていないと報じています。'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634de59-d380-46e3-9715-2e29b36e2a0d",
   "metadata": {},
   "source": [
    "# 上面内容的中文为：\n",
    "### 2008年中旬过后，俄罗斯堪察加半岛附近发生了一场里氏8.8级的大地震。震源地附近区域观测到了强烈的震动。路透社援引俄罗斯当地当局的消息称，幼儿园建筑出现了受损情况。此外，机场等地有几人受了轻伤。美联社报道称，这是自2011年3月东日本大地震以来，被认为是世界最大规模的地震。从全球范围来看，迄今为止观测到的比这场地震更强的地震仅有几次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21871933-675d-4faf-a8d5-f1c4a0232f70",
   "metadata": {},
   "source": [
    "## Homework\n",
    "### 使用完整的数据集训练，对比 Train Loss 和 Validation Loss 变化。训练完成后，使用测试集进行模型评估.\n",
    "### [Optional]使用其他语种（如：德语、法语等）的数据集进行微调训练，并进行模型评估模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6483918-0159-4518-b871-0b1b4144cd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e6063-1531-453b-9f35-5787d22da8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01691a-4d19-47b7-b4e8-041156d7488c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b846a-e3f4-4e1a-9db4-8def405d4a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ec75ae-e335-4fef-aafe-354a28785118",
   "metadata": {},
   "source": [
    "# 使用测试集进行完整的模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6900865-527e-4558-b64f-52a2e2a52b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Japanese\"          \n",
    "language_abbr = \"ja\"      \n",
    "language_decode = \"japanese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e886e259-9c82-465a-9a5e-684aaf8a769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "import torch \n",
    "\n",
    "model_dir = \"models/whisper-large-v2-asr-int8-fr\"\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=False, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40700243-3694-4c57-b650-733e75e07c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abb2110f4284c819da330a27c55266f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------- 加载并预处理测试集 ---------------\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 1. 加载测试集（使用Common Voice的测试集拆分）\n",
    "common_voice_test = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/mnt/e/aistudy_workspace/week02/ja_dataset/test/*.parquet\", \n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# 随机打乱测试集并仅选择前5条样本\n",
    "common_voice_test = common_voice_test.shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3e06e3-0a33-4b62-866f-6e81c7346310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 预处理测试集（与训练集保持一致）\n",
    "# 移除无关列\n",
    "common_voice_test = common_voice_test.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00ce0c6-46f1-4c2c-9e2f-9e97fee8ee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9a9bf8-2886-4a4d-8c1f-5c30ef1a07d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157d60a126744b2e9bfab17ad590b6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 音频降采样到16kHz\n",
    "common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "# 应用数据预处理函数（与训练时相同）\n",
    "def prepare_test_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    return batch\n",
    "test_dataset = common_voice_test.map(prepare_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf26546-4b89-4540-ad4c-00ae3833cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换原来的评估指标加载部分\n",
    "from transformers import WhisperProcessor\n",
    "import jiwer \n",
    "import torch\n",
    "\n",
    "def quick_evaluate(model, test_dataset, processor, batch_size=2):\n",
    "    \"\"\"兼容所有jiwer版本的评估函数，不使用任何关键字参数\"\"\"\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    # 批量处理样本\n",
    "    for i in range(0, len(test_dataset), batch_size):\n",
    "        batch = test_dataset[i:i+batch_size]\n",
    "        inputs = {\"input_features\": batch[\"input_features\"]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_features=torch.tensor(inputs[\"input_features\"]).to(model.device),\n",
    "                max_new_tokens=255\n",
    "            )\n",
    "        \n",
    "        # 解码预测结果和参考文本\n",
    "        predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        references = [sentence for sentence in batch[\"sentence\"]]\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_references.extend(references)\n",
    "        print(f\"已处理 {min(i+batch_size, len(test_dataset))}/{len(test_dataset)} 条样本\")\n",
    "    \n",
    "    # 关键修正：完全按位置传递参数（旧版jiwer仅支持这种方式）\n",
    "    # 第一个参数：真实标签（all_references）\n",
    "    # 第二个参数：预测结果（all_predictions）\n",
    "    wer = jiwer.wer(all_references, all_predictions)\n",
    "    cer = jiwer.cer(all_references, all_predictions)\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer,\n",
    "        \"predictions\": all_predictions,\n",
    "        \"references\": all_references\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e1d05c-696b-44ce-a6a4-96a0756f623e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- 执行评估并输出结果 ---------------\n",
    "peft_model = peft_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46d910a-9c9e-4255-982a-a1cb4557b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 8/10 条样本\n",
      "已处理 10/10 条样本\n"
     ]
    }
   ],
   "source": [
    "# 执行评估\n",
    "evaluation_results = quick_evaluate(\n",
    "    model=peft_model,\n",
    "    test_dataset=test_dataset,\n",
    "    processor=processor,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d6b49b-723f-4980-9e3d-6274bfe9d2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试集评估结果：\n",
      "词错误率（WER）：2.6000\n",
      "字符错误率（CER）：1.5376\n"
     ]
    }
   ],
   "source": [
    "# 输出关键指标\n",
    "print(f\"\\n测试集评估结果：\")\n",
    "print(f\"词错误率（WER）：{evaluation_results['wer']:.4f}\")  # 越低越好，0表示完全匹配\n",
    "print(f\"字符错误率（CER）：{evaluation_results['cer']:.4f}\")  # 越低越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1d8901-776d-44ed-81d0-38d18e63bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "随机样本对比：\n",
      "\n",
      "真实文本：而して斯く我々が何処までも\n",
      "预测文本：しかしてかく、われわれがどこまでも。\n",
      "\n",
      "真实文本：部屋に箱が六つ置いてあります。\n",
      "预测文本：へやにはこがむつおえてあります。\n",
      "\n",
      "真实文本：今晩友達がうちに泊まります。\n",
      "预测文本：こんばんトモタチがうちにとまります。\n",
      "\n",
      "真实文本：本能による適応は直接的である。\n",
      "预测文本：本能による適応は直接的である。\n",
      "\n",
      "真实文本：そっと階段をのぼった。\n",
      "预测文本：そっと階段を登った。\n"
     ]
    }
   ],
   "source": [
    "# 随机打印5个样本的预测与真实结果对比\n",
    "sample_indices = np.random.choice(len(evaluation_results[\"predictions\"]), 5, replace=False)\n",
    "print(\"\\n随机样本对比：\")\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\n真实文本：{evaluation_results['references'][idx]}\")\n",
    "    print(f\"预测文本：{evaluation_results['predictions'][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20263a0-14e8-4666-8fd4-0ca8eb65b0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
