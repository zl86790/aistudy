{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba23f45-1299-4631-b74f-21aab34baad3",
   "metadata": {},
   "source": [
    "# PEFT åº“ LoRA å®æˆ˜ - OpenAI Whisper-large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c1f130-4eca-42c7-b77d-a96b841b8815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU å¯ç”¨ï¼Œå½“å‰ GPU æ•°é‡: 1\n",
      "å½“å‰ä½¿ç”¨çš„ GPU: Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "import torch  # å¯¼å…¥ PyTorch åº“\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU å¯ç”¨ï¼Œå½“å‰ GPU æ•°é‡: {torch.cuda.device_count()}\")  # è¾“å‡ºå¯ç”¨ GPU çš„æ•°é‡\n",
    "    print(f\"å½“å‰ä½¿ç”¨çš„ GPU: {torch.cuda.get_device_name(0)}\")      # è¾“å‡ºå½“å‰ä½¿ç”¨çš„ GPU åç§°\n",
    "else:\n",
    "    print(\"æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPU\")  # æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨çš„ GPU æ—¶è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e56bc91-1ffe-46b4-b434-16b03bb76d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65d54d8-d501-406f-a3de-de19246bda11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.enabled)  # True è¡¨ç¤º cuDNN å¯ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7017244f-310a-4e45-9d56-a80557ca5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
    "\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db68a4ef-aef4-447a-bc71-1a110e470ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†å¤§å°ï¼š 29383\n",
      "è®­ç»ƒé›†ç¬¬ä¸€æ¡æ•°æ®ï¼š {'client_id': 'bcb4464171113dd9b51f371c3eecea06771fde83e7e3239ad0516469c6dcdf80170d26c7d1b1ef2476c45b51bfb4ee5549f07d7002bcfcec9b371a30c873b92d', 'path': 'zh-CN_train_0/common_voice_zh-CN_18551060.mp3', 'audio': <datasets.features._torchcodec.AudioDecoder object at 0x7fb4ddad7550>, 'sentence': 'å·´é¡¿æ˜¯ä½äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·é˜¿é©¬å¤šå°”å¿çš„ä¸€ä¸ªéå»ºåˆ¶åœ°åŒºã€‚', 'up_votes': 2, 'down_votes': 0, 'age': 'twenties', 'gender': 'male', 'accent': '', 'locale': 'zh-CN', 'segment': '', 'variant': ''}\n",
      "éªŒè¯é›†å¤§å°ï¼š 10624\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/home/pactera/aistudy/week02/Lizhe/datasets/zh/train/*.parquet\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "common_voice[\"validation\"] = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/home/pactera/aistudy/week02/Lizhe/datasets/zh/validation/*.parquet\",\n",
    "    split=\"train\"  # è¿™é‡Œç”¨ train æ ‡è®°ï¼Œåç»­é€šè¿‡ DatasetDict åŒºåˆ†\n",
    ")\n",
    "\n",
    "# éªŒè¯åŠ è½½ç»“æœ\n",
    "print(\"è®­ç»ƒé›†å¤§å°ï¼š\", len(common_voice[\"train\"]))\n",
    "print(\"è®­ç»ƒé›†ç¬¬ä¸€æ¡æ•°æ®ï¼š\", common_voice[\"train\"][0])\n",
    "print(\"éªŒè¯é›†å¤§å°ï¼š\", len(common_voice[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec81c31-5333-4370-b2ec-1d5627ea384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # æ£€æŸ¥æ˜¯å¦æ”¯æŒ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955d97f1-4271-4df9-9912-cbf9891daddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 29383\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 10624\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3965f43a-7dd2-4406-b886-13ea52f7c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç‰¹å¾æå–å™¨\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨ï¼Œå¯ä»¥æŒ‡å®šè¯­è¨€å’Œä»»åŠ¡ä»¥è·å¾—æœ€é€‚åˆç‰¹å®šéœ€æ±‚çš„åˆ†è¯å™¨é…ç½®\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤„ç†å™¨ï¼Œå¤„ç†å™¨é€šå¸¸ç»“åˆäº†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼Œä¸ºç‰¹å®šä»»åŠ¡æä¾›ä¸€ç«™å¼çš„æ•°æ®é¢„å¤„ç†\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d975f8f-d597-468e-9374-a21890f13265",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7be443-2c74-4f50-93f7-4c1dffb8a8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': <datasets.features._torchcodec.AudioDecoder at 0x7fb45e3b6b50>,\n",
       " 'sentence': 'å·´é¡¿æ˜¯ä½äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·é˜¿é©¬å¤šå°”å¿çš„ä¸€ä¸ªéå»ºåˆ¶åœ°åŒºã€‚',\n",
       " 'variant': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541d518-be42-46f2-8447-22fd51d4f292",
   "metadata": {},
   "source": [
    "### é™é‡‡æ ·éŸ³é¢‘æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a55ebb2-e2b4-4daa-8ee2-36033e1bf1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f57abc-0902-452a-908f-4859e27e9ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': <datasets.features._torchcodec.AudioDecoder at 0x7fb45e3b7790>,\n",
       " 'sentence': 'å·´é¡¿æ˜¯ä½äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·é˜¿é©¬å¤šå°”å¿çš„ä¸€ä¸ªéå»ºåˆ¶åœ°åŒºã€‚',\n",
       " 'variant': ''}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling_rate ä» 48KHZ é™ä¸º 16KHZ\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed360f-e442-4c08-bc8e-84b0140dac3e",
   "metadata": {},
   "source": [
    "### æ•´åˆä»¥ä¸Šæ•°æ®å¤„ç†ä¸ºä¸€ä¸ªå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ebb106d-661c-4ec0-ada7-e0bfbd444fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02addecd-ae20-4a22-9129-fc0ce02535fd",
   "metadata": {},
   "source": [
    "### æ•°æ®æŠ½æ ·ï¼ˆæ¼”ç¤ºéœ€è¦ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8de5870-328d-431d-8311-37e93106e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_common_voice = DatasetDict()\n",
    "\n",
    "all_common_voice[\"train\"] = common_voice[\"train\"].shuffle(seed=16)\n",
    "all_common_voice[\"validation\"] = common_voice[\"validation\"].shuffle(seed=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab10b927-189a-4e2c-858c-d9f846ba8398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant'],\n",
       "        num_rows: 29383\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant'],\n",
       "        num_rows: 10624\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91075b58-1d85-4ebb-bcce-92aa45e85d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŠ½æ ·æ•°æ®å¤„ç†\n",
    "tokenized_common_voice = all_common_voice.map(prepare_dataset)\n",
    "\n",
    "# å®Œæ•´æ•°æ®è®­ç»ƒï¼Œå°è¯•å¼€å¯ `num_proc=8` å‚æ•°å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆå¦‚é˜»å¡æ— æ³•è¿è¡Œï¼Œåˆ™ä¸ä½¿ç”¨æ­¤å‚æ•°ï¼‰\n",
    "# tokenized_common_voice = common_voice.map(prepare_dataset, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a622538-2c2f-4217-b084-528728276e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant', 'input_features', 'labels'],\n",
       "        num_rows: 29383\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence', 'variant', 'input_features', 'labels'],\n",
       "        num_rows: 10624\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d792ba9-cac3-4278-b2d7-d9366b97b1d2",
   "metadata": {},
   "source": [
    "# è‡ªå®šä¹‰è¯­éŸ³æ•°æ®æ•´ç†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e5c0e9f-2ff2-4efe-9dc5-188a1509065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡çš„æ•°æ®æ•´ç†å™¨ç±»\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # å¤„ç†å™¨ç»“åˆäº†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨\n",
    "\n",
    "    # æ•´ç†å™¨å‡½æ•°ï¼Œå°†ç‰¹å¾åˆ—è¡¨å¤„ç†æˆä¸€ä¸ªæ‰¹æ¬¡\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # ä»ç‰¹å¾åˆ—è¡¨ä¸­æå–è¾“å…¥ç‰¹å¾ï¼Œå¹¶å¡«å……ä»¥ä½¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„å½¢çŠ¶\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # ä»ç‰¹å¾åˆ—è¡¨ä¸­æå–æ ‡ç­¾ç‰¹å¾ï¼ˆæ–‡æœ¬ä»¤ç‰Œï¼‰ï¼Œå¹¶è¿›è¡Œå¡«å……\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # ä½¿ç”¨-100æ›¿æ¢æ ‡ç­¾ä¸­çš„å¡«å……åŒºåŸŸï¼Œ-100é€šå¸¸ç”¨äºåœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥å¡«å……ä»¤ç‰Œ\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # å¦‚æœæ‰¹æ¬¡ä¸­çš„æ‰€æœ‰åºåˆ—éƒ½ä»¥å¥å­å¼€å§‹ä»¤ç‰Œå¼€å¤´ï¼Œåˆ™ç§»é™¤å®ƒ\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # å°†å¤„ç†è¿‡çš„æ ‡ç­¾æ·»åŠ åˆ°æ‰¹æ¬¡ä¸­\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch  # è¿”å›æœ€ç»ˆçš„æ‰¹æ¬¡ï¼Œå‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒæˆ–è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13cc7ff1-5548-4f2c-8393-ca316c5b5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨ç»™å®šçš„å¤„ç†å™¨å®ä¾‹åŒ–æ•°æ®æ•´ç†å™¨\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da4f93-51fe-4fd7-9d3f-ebc682e7382a",
   "metadata": {},
   "source": [
    "## æ¨¡å‹å‡†å¤‡\n",
    "### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆint8 ç²¾åº¦ï¼‰\n",
    "### ä½¿ç”¨ int8 ç²¾åº¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b55e5970-597d-48c1-a372-14485a4b3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=False, device_map=\"auto\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d9d28dc-44dc-4dc3-bb6d-5eed2d59536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9809ef1-a86d-4877-9107-8f639fee15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®æ¨¡å‹é…ç½®ä¸­çš„forced_decoder_idså±æ€§ä¸ºNone\n",
    "model.config.forced_decoder_ids = None  # è¿™é€šå¸¸ç”¨äºæŒ‡å®šåœ¨è§£ç ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰è¿‡ç¨‹ä¸­å¿…é¡»ä½¿ç”¨çš„ç‰¹å®štokençš„IDï¼Œè®¾ç½®ä¸ºNoneè¡¨ç¤ºæ²¡æœ‰è¿™æ ·çš„å¼ºåˆ¶è¦æ±‚\n",
    "\n",
    "# è®¾ç½®æ¨¡å‹é…ç½®ä¸­çš„suppress_tokensåˆ—è¡¨ä¸ºç©º\n",
    "model.config.suppress_tokens = []  # è¿™ç”¨äºæŒ‡å®šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åº”è¢«æŠ‘åˆ¶ï¼ˆä¸ç”Ÿæˆï¼‰çš„tokençš„åˆ—è¡¨ï¼Œè®¾ç½®ä¸ºç©ºåˆ—è¡¨è¡¨ç¤ºæ²¡æœ‰è¦æŠ‘åˆ¶çš„token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccd28a-1b25-4f11-92f4-de88fca5de64",
   "metadata": {},
   "source": [
    "# PEFT å¾®è°ƒå‰çš„æ¨¡å‹å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16437188-8c33-4727-829f-050e9ce2e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªLoraConfigå¯¹è±¡ï¼Œç”¨äºè®¾ç½®LoRAï¼ˆLow-Rank Adaptationï¼‰çš„é…ç½®å‚æ•°\n",
    "config = LoraConfig(\n",
    "    r=4,  # LoRAçš„ç§©ï¼Œå½±å“LoRAçŸ©é˜µçš„å¤§å°\n",
    "    lora_alpha=64,  # LoRAé€‚åº”çš„æ¯”ä¾‹å› å­\n",
    "    # æŒ‡å®šå°†LoRAåº”ç”¨åˆ°çš„æ¨¡å‹æ¨¡å—ï¼Œé€šå¸¸æ˜¯attentionå’Œå…¨è¿æ¥å±‚çš„æŠ•å½±ã€‚\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # åœ¨LoRAæ¨¡å—ä¸­ä½¿ç”¨çš„dropoutç‡\n",
    "    bias=\"none\",  # è®¾ç½®biasçš„ä½¿ç”¨æ–¹å¼ï¼Œè¿™é‡Œæ²¡æœ‰ä½¿ç”¨bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8bc35fa-eaa2-4429-8a93-8d1d6ac27ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7555101-c552-4ac6-9c14-8175f4520f56",
   "metadata": {},
   "source": [
    "# LoRA Adapter é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ae41a-90b3-4d0d-bb29-49e581bddb7a",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨get_peft_modelå‡½æ•°å’Œç»™å®šçš„é…ç½®æ¥è·å–ä¸€ä¸ªPEFTæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc924606-5c64-4d9b-9d7e-21c46944b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'openai/whisper-large-v2' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7ed1dff-cbc0-4b0e-ae48-2c257fa47efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.1272\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å° LoRA å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹å‚æ•°\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29480e64-d9e2-441e-b996-d5877b894d33",
   "metadata": {},
   "source": [
    "### æ¨¡å‹è®­ç»ƒ\n",
    "#### Seq2SeqTrainingArguments è®­ç»ƒå‚æ•°\n",
    "å…³äºè®¾ç½®è®­ç»ƒæ­¥æ•°å’Œè¯„ä¼°æ­¥æ•°\n",
    "\n",
    "åŸºäº epochs è®¾ç½®ï¼š\n",
    "\n",
    "num_train_epochs=3,  # è®­ç»ƒçš„æ€»è½®æ•°\n",
    "evaluation_strategy=\"epoch\",  # è®¾ç½®è¯„ä¼°ç­–ç•¥ï¼Œè¿™é‡Œæ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œè¯„ä¼°\n",
    "warmup_steps=50,  # åœ¨è®­ç»ƒåˆæœŸå¢åŠ å­¦ä¹ ç‡çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ\n",
    "åŸºäº steps è®¾ç½®ï¼š\n",
    "\n",
    "max_steps=100, # è®­ç»ƒæ€»æ­¥æ•°\n",
    "evaluation_strategy=\"steps\", \n",
    "eval_steps=25, # è¯„ä¼°æ­¥æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "414d477a-f227-4792-9113-7270d1ea39e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# è®¾ç½®åºåˆ—åˆ°åºåˆ—æ¨¡å‹è®­ç»ƒçš„å‚æ•°\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # æŒ‡å®šæ¨¡å‹è¾“å‡ºå’Œä¿å­˜çš„ç›®å½•\n",
    "    per_device_train_batch_size=batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è®­ç»ƒæ‰¹é‡å¤§å°\n",
    "    learning_rate=1e-3,  # å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,  # è®­ç»ƒçš„æ€»è½®æ•°\n",
    "    evaluation_strategy=\"epoch\",  # è®¾ç½®è¯„ä¼°ç­–ç•¥ï¼Œè¿™é‡Œæ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œè¯„ä¼°\n",
    "    # warmup_steps=50,  # åœ¨è®­ç»ƒåˆæœŸå¢åŠ å­¦ä¹ ç‡çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ\n",
    "    # fp16=True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    per_device_eval_batch_size=batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è¯„ä¼°æ‰¹é‡å¤§å°\n",
    "    generation_max_length=32,  # ç”Ÿæˆä»»åŠ¡çš„æœ€å¤§é•¿åº¦\n",
    "    logging_steps=10,  # æŒ‡å®šæ—¥å¿—è®°å½•çš„æ­¥éª¤ï¼Œç”¨äºè·Ÿè¸ªè®­ç»ƒè¿›åº¦\n",
    "    remove_unused_columns=False,  # æ˜¯å¦åˆ é™¤ä¸ä½¿ç”¨çš„åˆ—ï¼Œä»¥å‡å°‘æ•°æ®å¤„ç†å¼€é”€\n",
    "    label_names=[\"labels\"],  # æŒ‡å®šæ ‡ç­¾åˆ—çš„åç§°ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=25,\n",
    "    fp16=True,  \n",
    "    fp16_opt_level=\"O1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf1ad2-8f38-4a25-8a38-db7c7ea1fc66",
   "metadata": {},
   "source": [
    "# å®ä¾‹åŒ– Seq2SeqTrainer è®­ç»ƒå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72b6b5e9-af00-4fce-818d-3b9aa33c137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor, \n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f137d169-24f3-4c1f-b7d0-053edf250388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  1 17:37:40 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.144                Driver Version: 570.144        CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 6000                Off |   00000000:B3:00.0  On |                  Off |\n",
      "| 33%   43C    P0             68W /  260W |    7455MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          888914      G   /usr/lib/xorg/Xorg                      369MiB |\n",
      "|    0   N/A  N/A          889040      G   ...c/gnome-remote-desktop-daemon          3MiB |\n",
      "|    0   N/A  N/A          889118      G   /usr/bin/gnome-shell                    164MiB |\n",
      "|    0   N/A  N/A         1046253      G   .../6565/usr/lib/firefox/firefox        449MiB |\n",
      "|    0   N/A  N/A         1056222      C   ...nda3/envs/aistudy5/bin/python       6358MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6f79dd3-1d74-4dba-9912-f4cf55b796ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7346' max='7346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7346/7346 3:14:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.361165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7346, training_loss=0.5035409991630182, metrics={'train_runtime': 11653.8523, 'train_samples_per_second': 2.521, 'train_steps_per_second': 0.63, 'total_flos': 6.2468321937408e+19, 'train_loss': 0.5035409991630182, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d827ed8-9f95-4793-afae-7d22b831de01",
   "metadata": {},
   "source": [
    "# ä¿å­˜ LoRA æ¨¡å‹(Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0db50c7-a8fa-44c7-b80d-49896c9d607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b3f5d15-8b28-4a97-9277-6200f0e178e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModel(\n",
       "      (base_model): LoraModel(\n",
       "        (model): WhisperForConditionalGeneration(\n",
       "          (model): WhisperModel(\n",
       "            (encoder): WhisperEncoder(\n",
       "              (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (embed_positions): Embedding(1500, 1280)\n",
       "              (layers): ModuleList(\n",
       "                (0-31): 32 x WhisperEncoderLayer(\n",
       "                  (self_attn): WhisperSdpaAttention(\n",
       "                    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (v_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (q_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  )\n",
       "                  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (activation_fn): GELUActivation()\n",
       "                  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (decoder): WhisperDecoder(\n",
       "              (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "              (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "              (layers): ModuleList(\n",
       "                (0-31): 32 x WhisperDecoderLayer(\n",
       "                  (self_attn): WhisperSdpaAttention(\n",
       "                    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (v_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (q_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  )\n",
       "                  (activation_fn): GELUActivation()\n",
       "                  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (encoder_attn): WhisperSdpaAttention(\n",
       "                    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (v_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (q_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  )\n",
       "                  (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8455bf-b4d8-44d9-a329-ad68f4841abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06f1cef9-68c8-460e-b5ae-4bc8d31f1a55",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æ¨ç†ï¼ˆå¯èƒ½éœ€è¦é‡å¯ Notebookï¼‰\n",
    "### ä½¿ç”¨ PeftModel åŠ è½½ LoRA å¾®è°ƒå Whisper æ¨¡å‹\n",
    "### ä½¿ç”¨ PeftConfig åŠ è½½ LoRA Adapter é…ç½®å‚æ•°ï¼Œä½¿ç”¨ PeftModel åŠ è½½å¾®è°ƒå Whisper æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecb38af-3197-43aa-aa62-d596e3750fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "language_decode = \"chinese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050f0e2d-ce9a-46c8-99f4-0b0dd951bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6b6447-42cd-48bb-916b-88e1984d74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config.base_model_name_or_path = model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28f44ec-3d63-402f-aa61-35be46035687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping={'base_model_class': 'PeftModel', 'parent_library': 'peft.peft_model'}, base_model_name_or_path='openai/whisper-large-v2', revision=None, task_type=None, inference_mode=True, r=4, target_modules={'q_proj', 'v_proj'}, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13bda33-9470-4545-afdc-5c75e7cc9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=False, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b912fa-eb0e-458b-9e0c-1dc262269fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26be9c-5497-4421-8206-563bac1b40de",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ Pipeline API éƒ¨ç½²å¾®è°ƒå Whisper å®ç°ä¸­æ–‡è¯­éŸ³è¯†åˆ«ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccf6448-9b32-4421-91ac-9f617e9f54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = \"zh_test.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37429273-2f30-4a17-8ce9-bad27334b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=peft_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e251b9-3ffd-4b0e-9afa-68cbf53fd944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1066041/3500902550.py:3: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, max_new_tokens=255)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9886e95-8779-4962-9eed-73ef7618d38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'é€™æ˜¯ä¸€å€‹ä¸­æ–‡æ¸¬è©¦'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f63d3a-b8f6-4f53-bd07-da860e0c158d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aistudy5)",
   "language": "python",
   "name": "aistudy5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
