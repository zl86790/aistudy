{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886ed769-07f9-474f-9f86-c9f1f17345e4",
   "metadata": {},
   "source": [
    "## 使用微调后的 LLaMA2-7B 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f9cf61-2994-48c7-9a42-150920397a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d15b15e-dee5-48ca-94d5-e79e06eb2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aistudy9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [07:26<00:00, 223.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_dir = \"/mnt/f/llama-2-7b-full-base\"\n",
    "lora_model_dir = \"/mnt/e/aistudy_workspace/week05/models/llama-7-int4-dolly-20250816_124440\"\n",
    "\n",
    "# 4bit 配置\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 先加载 base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# 再加载 LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7700c042-20cf-4ff1-8c40-a91cabeaaaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "The haiku is a traditional Japanese poem that has seventeen syllables and follows a specific structure. It is composed of three lines with five, then seven, then five syllables, such as:\n",
      "\n",
      "Flowers will bloom soon,\n",
      "Air warms and sun shines brightly,\n",
      "Pink leaves will fall next.\n",
      "\n",
      "Generated instruction:\n",
      "\n",
      "**Sample Response:**\n",
      "\n",
      "In a haiku, the first line is called the _kami-ita_ or \"little bridge,\" and it sets the scene or introduces the subject. The second line is called the _kusamono_ or \"describing word,\" and it provides more details about the subject. The third line is called the _kireji_ or \"cutting word,\" and it marks the end of the subject.\n",
      "\n",
      "**Sample\n",
      "Ground truth:\n",
      "What is a haiku?\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    " \n",
    " \n",
    "# 从hub加载数据集并得到一个样本\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    " \n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    " \n",
    "### Input:\n",
    "{sample['response']}\n",
    " \n",
    "### Response:\n",
    "\"\"\"\n",
    " \n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
